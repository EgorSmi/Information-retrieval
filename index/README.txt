1. Построение прямого индекса. Метод build_forward_index(self, path_to_dataset)
Из файла path_to_dataset считываю файлы .gz, декодирую из utf-8, читаю первые 4 байта, в которых указана длина текста. 
Читаю документ. В словарь прямого индекса по номеру doc_id (порядковый номер) заношу список слов, которые встречаются в тексте 
и прошли через регулярку r'\w+'. 
2. Построение обратного индекса build_backward_index(self). 
Создаю словарь. Ключи - слова из прямого индекса; значения - список doc_id, где эти слова встречаются.
3. Реализовал кодирование Фибонначи. 
4. Оптимизация прямого индекса forward_index_optimization(self, coder=Fibonnachi()).
Для каждой страницы в прямом индексе строим текст из неповторяющихся слов в этом тексте, 
потом кодируем получившийся текст codecs.encode()
5. Оптимизация обратного индекса backward_index_optimization(self, coder=Fibonnachi()). 
Терм в обратном индексе заменяем на его word_id (просто порядковый номер), кодируем кодом Фибонначи. 
Записываем posting list из промежутков между документами, кодируем posting list кодом Фибонначи
6. Построение словаря word_id <-> word make_dict(self). 
Каждому слову приписываем его порядковый номер; создаем словарь
7. Построение дерева запроса build_tree(self, query). 
build_tree(self, query) делит запрос на токены регуляркой r'\w+|[\(\)&\|!]' и запускаем рекурсивное построение дерева def build(self, tokens, pos)
def build(self, tokens, pos) уже находит в запросе оператор с самым низким приоритетом, делит запрос на две части и запускает рекурсию
для обеих частей пока операторов приоритета не останется. В общем реализовал алгоритм, который показывался на лекции на youtube.
8. Построение постинг листов в дереве build_posting_lists(self, index)
Изначально в дерево заносил операторы и слова запроса, теперь преобразую слова в списки документов (posting list), на которых эти слова встречаются.
Ищу слово в словаре, получаю word_id, получаю соответствующий posting list из обратного индекса. Преобразую posting list в привычный 
вид c doc_id, а не с промежутками между doc_id (см пункт 5)
9. Исполняем дерево потоково

Общий workflow
1. Запускаем bash ./index.sh - строится прямой, обратный индексы; оптимизируются; строится словарь.
index сохраняется в файл index.pkl
2. Запускаем bash ./searcher.sh - считывается индекс из файла index.pkl; строится дерево запроса и исполняется; 
выдается сам запрос, количество url, сами url.

Формат ввода для ./searcher.sh -- Ожидается N запросов, по одному на каждой строке, потому что в коде:

input_str = sys.stdin.readlines()
# цикл по всем запросам
for query in input_str:
    .....

Поэтому можно ввести N запросов и потом на новой строке ввести EOF (Ctrl + D)
Либо запускать скрипт как bash ./searcher.sh < requests.txt
где requests.txt - файл с запросами

Формат вывода:
Сам запрос

Количество url
Сами urls

Для ./index.sh ввод не нужен, файлы считываются из директории ./dataset